{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Colab Setup\n",
    "If you aren't using Colab, you can delete the following code cell. This is just to help students with mounting to Google Drive to access the other .py files and downloading the data, which is a little trickier on Colab than on your local machine using Jupyter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will be prompted with a window asking to grant permissions\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the path in your Google Drive in the string below. Note: do not escape slashes or spaces\n",
    "import os\n",
    "datadir = \"/content/assignment1\"\n",
    "if not os.path.exists(datadir):\n",
    "  !ln -s \"/content/drive/My Drive/YOUR PATH HERE/assignment1/\" $datadir\n",
    "os.chdir(datadir)\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading Fashion-MNIST\n",
    "import os\n",
    "os.chdir(os.path.join(datadir,\"fashion-mnist/\"))\n",
    "!chmod +x ./get_data.sh\n",
    "!./get_data.sh\n",
    "os.chdir(datadir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from data_process import get_FASHION_data, get_RICE_data\n",
    "from scipy.spatial import distance\n",
    "from models import Perceptron, SVM, Softmax, Logistic\n",
    "from kaggle_submission import output_submission_csv\n",
    "%matplotlib inline\n",
    "\n",
    "# For auto-reloading external modules\n",
    "# See http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Fashion-MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells we determine the number of images for each split and load the images.\n",
    "<br /> \n",
    "TRAIN_IMAGES + VAL_IMAGES = (0, 60000]\n",
    ", TEST_IMAGES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change these numbers for experimentation\n",
    "# For submission we will use the default values \n",
    "TRAIN_IMAGES = 50000\n",
    "VAL_IMAGES = 10000\n",
    "normalize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_FASHION_data(TRAIN_IMAGES, VAL_IMAGES, normalize=normalize)\n",
    "X_train_fashion, y_train_fashion = data['X_train'], data['y_train']\n",
    "X_val_fashion, y_val_fashion = data['X_val'], data['y_val']\n",
    "X_test_fashion, y_test_fashion = data['X_test'], data['y_test']\n",
    "n_class_fashion = len(np.unique(y_test_fashion))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples:  10911\n",
      "Number of val samples:  3637\n",
      "Number of test samples:  3637\n"
     ]
    }
   ],
   "source": [
    "# loads train / test / val splits of 80%, 20%, 20% \n",
    "data = get_RICE_data()\n",
    "X_train_RICE, y_train_RICE = data['X_train'], data['y_train']\n",
    "X_val_RICE, y_val_RICE = data['X_val'], data['y_val']\n",
    "X_test_RICE, y_test_RICE = data['X_test'], data['y_test']\n",
    "n_class_RICE = len(np.unique(y_test_RICE))\n",
    "\n",
    "print(\"Number of train samples: \", X_train_RICE.shape[0])\n",
    "print(\"Number of val samples: \", X_val_RICE.shape[0])\n",
    "print(\"Number of test samples: \", X_test_RICE.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes how well your model performs using accuracy as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(pred, y_test):\n",
    "    return np.sum(y_test == pred) / len(y_test) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron has 2 hyperparameters, \"learning rate\" and \"number of epochs\" that you can experiment with:\n",
    "### Hyperparameter 1: Learning rate\n",
    "The learning rate controls how much we change the current weights of the classifier during each update. We set it at a default value of 0.5, but you should experiment with different values. Here is a guide to help you find a right learning rate: \n",
    "- Try values ranging from 0.0005 to 5.0 to see the impact on model accuracy. \n",
    "- If the loss oscillates a lot or the loss diverges, the learning rate may be too high. Try decreasing it by a factor of 10 (e.g. from 0.5 to 0.05). \n",
    "- If the loss decreases very slowly, the learning rate may be too low and the training can take a long time. Try increasing it by a factor of 10.\n",
    "- You can also try adding a learning rate decay to slowly reduce the learning rate over each training epoch. For example, multiply the learning rate by 0.95 after each epoch.\n",
    "- Plot training and validation accuracy over epochs for different learning rates. This will help you visualize the impact of the learning rate.\n",
    "- [Here](https://towardsdatascience.com/https-medium-com-dashingaditya-rakhecha-understanding-learning-rate-dd5da26bb6de) is a detailed guide to learning rate.\n",
    "\n",
    "### Hyperparameter 2: Number of Epochs\n",
    "- An epoch is a complete iterative pass over the training dataset. During an epoch we predict a label using the classifier and then update the weights of the classifier according to the perceptron update rule for each sample in the training set. \n",
    "- By observing the training and validation accuracy graphs, you can determine if the model has converged or not. Try increasing the number of epochs if the model is not converging or has low accuracy. \n",
    "- You should try different values for the number of training epochs and report your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement the Perceptron classifier in the **models/perceptron.py** script.\n",
    "\n",
    "The following code: \n",
    "- Creates an instance of the Perceptron classifier class \n",
    "- An instance of the Perceptron class is trained on the training data using its train method.\n",
    "- We use the predict method to find the training accuracy as well as the testing accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Perceptron on Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2779377.0010659765\n",
      "Epoch 10, Loss: 1792346.1921598765\n",
      "Epoch 20, Loss: 1546681.094628565\n",
      "Epoch 30, Loss: 1415226.6069011237\n",
      "Epoch 40, Loss: 1289434.5263538193\n",
      "Epoch 50, Loss: 1204798.4813808065\n",
      "Epoch 60, Loss: 1110442.3318227443\n",
      "Epoch 70, Loss: 1045596.8318219888\n",
      "Epoch 80, Loss: 984950.9086845838\n",
      "Epoch 90, Loss: 920220.1969339837\n",
      "Epoch 100, Loss: 876976.6980903608\n",
      "Epoch 110, Loss: 835661.638616229\n",
      "Epoch 120, Loss: 792924.4751429242\n",
      "Epoch 130, Loss: 756005.447527989\n",
      "Epoch 140, Loss: 725789.9169374912\n",
      "Epoch 150, Loss: 699173.3372378723\n",
      "Epoch 160, Loss: 674258.6263018169\n",
      "Epoch 170, Loss: 646202.4329279001\n",
      "Epoch 180, Loss: 622773.8433814138\n",
      "Epoch 190, Loss: 599981.5630557359\n",
      "Epoch 200, Loss: 579494.1771371483\n",
      "Epoch 210, Loss: 564540.088753766\n",
      "Epoch 220, Loss: 540920.8487486814\n",
      "Epoch 230, Loss: 528793.4868908674\n",
      "Epoch 240, Loss: 514170.72197828203\n",
      "Epoch 250, Loss: 495889.93677688704\n",
      "Epoch 260, Loss: 484065.0155946028\n",
      "Epoch 270, Loss: 469263.77363839286\n",
      "Epoch 280, Loss: 460408.7571546182\n",
      "Epoch 290, Loss: 448813.0164603036\n",
      "Epoch 300, Loss: 437550.38865123526\n",
      "Epoch 310, Loss: 424086.35549642256\n",
      "Epoch 320, Loss: 416846.0079814372\n",
      "Epoch 330, Loss: 402366.93642037024\n",
      "Epoch 340, Loss: 396111.80256709666\n",
      "Epoch 350, Loss: 386431.1140500195\n",
      "Epoch 360, Loss: 376619.119820516\n",
      "Epoch 370, Loss: 371390.34459106764\n",
      "Epoch 380, Loss: 360371.52206135466\n",
      "Epoch 390, Loss: 354494.73066352564\n",
      "Epoch 400, Loss: 347064.8648777592\n",
      "Epoch 410, Loss: 342194.0700690493\n",
      "Epoch 420, Loss: 335031.0110963823\n",
      "Epoch 430, Loss: 327315.48618013866\n",
      "Epoch 440, Loss: 323529.7840530214\n",
      "Epoch 450, Loss: 317322.1386669396\n",
      "Epoch 460, Loss: 311602.61839957844\n",
      "Epoch 470, Loss: 303422.3052630069\n",
      "Epoch 480, Loss: 299086.2978465475\n",
      "Epoch 490, Loss: 293793.0766142928\n",
      "Epoch 500, Loss: 290136.50945453945\n",
      "Epoch 510, Loss: 285195.0827894204\n",
      "Epoch 520, Loss: 283151.1759262848\n",
      "Epoch 530, Loss: 276520.3770035275\n",
      "Epoch 540, Loss: 269568.95300616376\n",
      "Epoch 550, Loss: 267171.48583038454\n",
      "Epoch 560, Loss: 265159.9586084411\n",
      "Epoch 570, Loss: 258812.22338034515\n",
      "Epoch 580, Loss: 255045.80400177493\n",
      "Epoch 590, Loss: 252605.47752607864\n",
      "Epoch 600, Loss: 249229.95276941307\n",
      "Epoch 610, Loss: 245931.0975230749\n",
      "Epoch 620, Loss: 241912.70892795475\n",
      "Epoch 630, Loss: 237213.3067348981\n",
      "Epoch 640, Loss: 236454.49302115958\n",
      "Epoch 650, Loss: 232840.77252184082\n",
      "Epoch 660, Loss: 229256.0393286219\n",
      "Epoch 670, Loss: 227179.10835294498\n",
      "Epoch 680, Loss: 223386.79100243657\n",
      "Epoch 690, Loss: 219779.4122425399\n",
      "Epoch 700, Loss: 218163.95977914147\n",
      "Epoch 710, Loss: 215080.2853038525\n",
      "Epoch 720, Loss: 212712.69040485448\n",
      "Epoch 730, Loss: 210643.48182439498\n",
      "Epoch 740, Loss: 208992.93350398346\n",
      "Epoch 750, Loss: 204102.1957220851\n",
      "Epoch 760, Loss: 202551.26619831158\n",
      "Epoch 770, Loss: 201500.10710504887\n",
      "Epoch 780, Loss: 199182.874167679\n",
      "Epoch 790, Loss: 196008.95612346206\n",
      "Epoch 800, Loss: 193161.3994172576\n",
      "Epoch 810, Loss: 191594.03786622972\n",
      "Epoch 820, Loss: 189643.17395512422\n",
      "Epoch 830, Loss: 187646.84071287635\n",
      "Epoch 840, Loss: 187053.61572832687\n",
      "Epoch 850, Loss: 183788.11449126664\n",
      "Epoch 860, Loss: 181194.90880805312\n",
      "Epoch 870, Loss: 179247.78982034442\n",
      "Epoch 880, Loss: 177379.58731067454\n",
      "Epoch 890, Loss: 177729.4501003932\n",
      "Epoch 900, Loss: 175778.03764061086\n",
      "Epoch 910, Loss: 173017.47096814704\n",
      "Epoch 920, Loss: 171954.85465183647\n",
      "Epoch 930, Loss: 169672.3012227553\n",
      "Epoch 940, Loss: 170248.5073684646\n",
      "Epoch 950, Loss: 165574.69276417885\n",
      "Epoch 960, Loss: 166039.3393538038\n",
      "Epoch 970, Loss: 164880.85005251053\n",
      "Epoch 980, Loss: 162640.35578422304\n",
      "Epoch 990, Loss: 161475.76613189516\n",
      "Epoch 1000, Loss: 160507.19398998795\n",
      "Epoch 1010, Loss: 157342.38217261588\n",
      "Epoch 1020, Loss: 157192.50384067025\n",
      "Epoch 1030, Loss: 155145.81596427344\n",
      "Epoch 1040, Loss: 152838.75987772897\n",
      "Epoch 1050, Loss: 151607.82285058108\n",
      "Epoch 1060, Loss: 151200.06134093998\n",
      "Epoch 1070, Loss: 150604.7654570309\n",
      "Epoch 1080, Loss: 149284.15445551154\n",
      "Epoch 1090, Loss: 146996.07128654057\n",
      "Epoch 1100, Loss: 146841.7525420864\n",
      "Epoch 1110, Loss: 145237.8420009544\n",
      "Epoch 1120, Loss: 143537.2844874094\n",
      "Epoch 1130, Loss: 144366.11836274556\n",
      "Epoch 1140, Loss: 141649.2122269366\n",
      "Epoch 1150, Loss: 139448.98365502863\n",
      "Epoch 1160, Loss: 139101.35704288757\n",
      "Epoch 1170, Loss: 139171.77914408187\n",
      "Epoch 1180, Loss: 138327.03339810774\n",
      "Epoch 1190, Loss: 137149.68790491496\n",
      "Epoch 1200, Loss: 134948.48487732312\n",
      "Epoch 1210, Loss: 134315.25283714803\n",
      "Epoch 1220, Loss: 133769.27367039642\n",
      "Epoch 1230, Loss: 132163.9422431383\n",
      "Epoch 1240, Loss: 131292.68323909474\n",
      "Epoch 1250, Loss: 129366.27319120338\n",
      "Epoch 1260, Loss: 130057.79771431682\n",
      "Epoch 1270, Loss: 128469.06932300293\n",
      "Epoch 1280, Loss: 128300.2612436134\n",
      "Epoch 1290, Loss: 126391.48166358582\n",
      "Epoch 1300, Loss: 126584.31192551857\n",
      "Epoch 1310, Loss: 125892.82317722241\n",
      "Epoch 1320, Loss: 124137.08143116486\n",
      "Epoch 1330, Loss: 122831.35736823293\n",
      "Epoch 1340, Loss: 122923.83632872475\n",
      "Epoch 1350, Loss: 121111.96846354565\n",
      "Epoch 1360, Loss: 121243.0549986219\n",
      "Epoch 1370, Loss: 120042.91388219885\n",
      "Epoch 1380, Loss: 119703.94305628439\n",
      "Epoch 1390, Loss: 118994.26183692823\n",
      "Epoch 1400, Loss: 117278.76667207417\n",
      "Epoch 1410, Loss: 117016.78310788689\n",
      "Epoch 1420, Loss: 115897.26306424457\n",
      "Epoch 1430, Loss: 117160.23065090204\n",
      "Epoch 1440, Loss: 115558.96029268646\n",
      "Epoch 1450, Loss: 114827.91787103996\n",
      "Epoch 1460, Loss: 114221.49689565995\n",
      "Epoch 1470, Loss: 113388.61038520854\n",
      "Epoch 1480, Loss: 112570.00384841058\n",
      "Epoch 1490, Loss: 111223.97130926342\n"
     ]
    }
   ],
   "source": [
    "lr = 1.25\n",
    "n_epochs = 1500\n",
    "percept_fashion = Perceptron(n_class_fashion, lr, n_epochs)\n",
    "\n",
    "# - Refer to hints in the train method of the Perceptron class in \"models/perceptron.py\"\n",
    "percept_fashion.train(X_train_fashion, y_train_fashion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "print(list(set(y_train_fashion)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'percept_fashion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred_percept \u001b[38;5;241m=\u001b[39m \u001b[43mpercept_fashion\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_train_fashion)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe training accuracy is given by: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (get_acc(pred_percept, y_train_fashion)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'percept_fashion' is not defined"
     ]
    }
   ],
   "source": [
    "pred_percept = percept_fashion.predict(X_train_fashion)\n",
    "print('The training accuracy is given by: %f' % (get_acc(pred_percept, y_train_fashion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Perceptron on Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation accuracy is given by: 75.400000\n"
     ]
    }
   ],
   "source": [
    "pred_percept = percept_fashion.predict(X_val_fashion)\n",
    "print('The validation accuracy is given by: %f' % (get_acc(pred_percept, y_val_fashion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Perceptron on Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing accuracy is given by: 74.360000\n"
     ]
    }
   ],
   "source": [
    "pred_percept = percept_fashion.predict(X_test_fashion)\n",
    "print('The testing accuracy is given by: %f' % (get_acc(pred_percept, y_test_fashion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron_Fashion-MNIST Kaggle Submission\n",
    "\n",
    "Once you are satisfied with your solution and test accuracy, output a file to submit your test set predictions to the Kaggle for Assignment 1 Fashion-MNIST. Use the following code to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_submission_csv('kaggle/perceptron_submission_fashion.csv', percept_fashion.predict(X_test_fashion))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Perceptron on Rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 27614223.040922128\n"
     ]
    }
   ],
   "source": [
    "lr = 0.5\n",
    "n_epochs = 10\n",
    "\n",
    "percept_RICE = Perceptron(n_class_RICE, lr, n_epochs)\n",
    "percept_RICE.train(X_train_RICE, y_train_RICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_percept = percept_RICE.predict(X_train_RICE)\n",
    "print('The training accuracy is given by: %f' % (get_acc(pred_percept, y_train_RICE)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Perceptron on Rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_percept = percept_RICE.predict(X_val_RICE)\n",
    "print('The validation accuracy is given by: %f' % (get_acc(pred_percept, y_val_RICE)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Perceptron on Rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_percept = percept_RICE.predict(X_test_RICE)\n",
    "print('The testing accuracy is given by: %f' % (get_acc(pred_percept, y_test_RICE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (with SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will implement a \"soft margin\" SVM. In this formulation you will maximize the margin between positive and negative training examples and penalize margin violations using a hinge loss.\n",
    "\n",
    "We will optimize the SVM loss using SGD. This means you must compute the loss function with respect to model weights. You will use this gradient to update the model weights.\n",
    "\n",
    "SVM optimized with SGD has 3 hyperparameters that you can experiment with:\n",
    "- **Learning rate** - similar to as defined above in Perceptron, this parameter scales by how much the weights are changed according to the calculated gradient update. \n",
    "- **Epochs** - similar to as defined above in Perceptron.\n",
    "- **Regularization constant** - Hyperparameter to determine the strength of regularization. In this case it is a coefficient on the term which maximizes the margin. You could try different values. The default value is set to 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will implement the SVM using SGD in the **models/svm.py**\n",
    "\n",
    "The following code: \n",
    "- Creates an instance of the SVM classifier class \n",
    "- The train function of the SVM class is trained on the training data\n",
    "- We use the predict function to find the training accuracy as well as the testing accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVM on Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the following three hyperparameters to achieve the best accuracy:\n",
    "lr = 0.75\n",
    "n_epochs = 1000\n",
    "reg_const = 0.05\n",
    "\n",
    "svm_fashion = SVM(n_class_fashion, lr, n_epochs, reg_const)\n",
    "\n",
    "# Refer to hints in the calculate_gradient and train methods of the SVM class in \"models/svm.py\"\n",
    "svm_fashion.train(X_train_fashion, y_train_fashion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is given by: 77.736000\n"
     ]
    }
   ],
   "source": [
    "pred_svm = svm_fashion.predict(X_train_fashion)\n",
    "print('The training accuracy is given by: %f' % (get_acc(pred_svm, y_train_fashion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate SVM on Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation accuracy is given by: 77.360000\n"
     ]
    }
   ],
   "source": [
    "pred_svm = svm_fashion.predict(X_val_fashion)\n",
    "print('The validation accuracy is given by: %f' % (get_acc(pred_svm, y_val_fashion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test SVM on Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing accuracy is given by: 76.640000\n"
     ]
    }
   ],
   "source": [
    "pred_svm = svm_fashion.predict(X_test_fashion)\n",
    "print('The testing accuracy is given by: %f' % (get_acc(pred_svm, y_test_fashion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM_Fashion-MNIST Kaggle Submission\n",
    "\n",
    "Once you are satisfied with your solution and test accuracy output a file to submit your test set predictions to the Kaggle for Assignment 1 Fashion-MNIST. Use the following code to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_submission_csv('kaggle/svm_submission_fashion.csv', svm_fashion.predict(X_test_fashion))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVM on Rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the following three hyperparameters to achieve the best accuracy: \n",
    "lr = 0.5\n",
    "n_epochs = 10\n",
    "reg_const = 0.05\n",
    "\n",
    "svm_RICE = SVM(n_class_RICE, lr, n_epochs, reg_const)\n",
    "svm_RICE.train(X_train_RICE, y_train_RICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is given by: 45.220420\n"
     ]
    }
   ],
   "source": [
    "pred_svm = svm_RICE.predict(X_train_RICE)\n",
    "print('The training accuracy is given by: %f' % (get_acc(pred_svm, y_train_RICE)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate SVM on Rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation accuracy is given by: 43.854825\n"
     ]
    }
   ],
   "source": [
    "pred_svm = svm_RICE.predict(X_val_RICE)\n",
    "print('The validation accuracy is given by: %f' % (get_acc(pred_svm, y_val_RICE)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test SVM on Rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing accuracy is given by: 45.944460\n"
     ]
    }
   ],
   "source": [
    "pred_svm = svm_RICE.predict(X_test_RICE)\n",
    "print('The testing accuracy is given by: %f' % (get_acc(pred_svm, y_test_RICE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classifier (with SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, you will train a Softmax classifier. This classifier consists of a linear function of the input data followed by a softmax function which outputs a vector of dimension C (number of classes) for each data point. Each entry of the softmax output vector corresponds to a confidence in one of the C classes, and like a probability distribution, the entries of the output vector sum to 1. We use a cross-entropy loss on this sotmax output to train the model. \n",
    "\n",
    "Check the following link as an additional resource on softmax classification: http://cs231n.github.io/linear-classify/#softmax\n",
    "\n",
    "Once again we will train the classifier with SGD. This means you need to compute the gradients of the softmax cross-entropy loss function according to the weights and update the weights using this gradient. Check the following link to help with implementing the gradient updates: https://deepnotes.io/softmax-crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax classifier has 3 hyperparameters that you can experiment with:\n",
    "- **Learning rate** - As above, this controls how much the model weights are updated with respect to their gradient.\n",
    "- **Number of Epochs** - As described for perceptron.\n",
    "- **Regularization constant** - Hyperparameter to determine the strength of regularization. In this case, we minimize the L2 norm of the model weights as regularization, so the regularization constant is a coefficient on the L2 norm in the combined cross-entropy and regularization objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will implement a softmax classifier using SGD in the **models/softmax.py**\n",
    "\n",
    "The following code: \n",
    "- Creates an instance of the Softmax classifier class \n",
    "- The train function of the Softmax class is trained on the training data\n",
    "- We use the predict function to find the training accuracy as well as the testing accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Softmax on Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.75\n",
    "n_epochs = 1000\n",
    "reg_const = 0.5\n",
    "\n",
    "softmax_fashion = Softmax(n_class_fashion, lr, n_epochs, reg_const)\n",
    "\n",
    "# Refer to hints in the calculate_gradient and train methods of the Softmax class in \"models/softmax.py\"\n",
    "softmax_fashion.train(X_train_fashion, y_train_fashion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is given by: 66.384000\n"
     ]
    }
   ],
   "source": [
    "pred_softmax = softmax_fashion.predict(X_train_fashion)\n",
    "print('The training accuracy is given by: %f' % (get_acc(pred_softmax, y_train_fashion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Softmax on Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation accuracy is given by: 65.510000\n"
     ]
    }
   ],
   "source": [
    "pred_softmax = softmax_fashion.predict(X_val_fashion)\n",
    "print('The validation accuracy is given by: %f' % (get_acc(pred_softmax, y_val_fashion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Softmax on Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing accuracy is given by: 65.540000\n"
     ]
    }
   ],
   "source": [
    "pred_softmax = softmax_fashion.predict(X_test_fashion)\n",
    "print('The testing accuracy is given by: %f' % (get_acc(pred_softmax, y_test_fashion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax_Fashion-MNIST Kaggle Submission\n",
    "\n",
    "Once you are satisfied with your solution and test accuracy output a file to submit your test set predictions to the Kaggle for Assignment 1 Fashion-MNIST. Use the following code to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_submission_csv('kaggle/softmax_submission_fashion.csv', softmax_fashion.predict(X_test_fashion))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Softmax on Rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5\n",
    "n_epochs = 10\n",
    "reg_const = 0.05\n",
    "\n",
    "softmax_RICE = Softmax(n_class_RICE, lr, n_epochs, reg_const)\n",
    "softmax_RICE.train(X_train_RICE, y_train_RICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_softmax = softmax_RICE.predict(X_train_RICE)\n",
    "print('The training accuracy is given by: %f' % (get_acc(pred_softmax, y_train_RICE)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Softmax on Rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_softmax = softmax_RICE.predict(X_val_RICE)\n",
    "print('The validation accuracy is given by: %f' % (get_acc(pred_softmax, y_val_RICE)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Softmax on Rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_softmax = softmax_RICE.predict(X_test_RICE)\n",
    "print('The testing accuracy is given by: %f' % (get_acc(pred_softmax, y_test_RICE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Classifier has 2 hyperparameters that you can experiment with:\n",
    "- **Learning rate** - similar to as defined above in Perceptron, this parameter scales by how much the weights are changed according to the calculated gradient update. \n",
    "- **Number of Epochs** - As described for perceptron.\n",
    "- **Threshold** - The decision boundary of the classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will implement the Logistic Classifier in the **models/logistic.py**\n",
    "\n",
    "The following code: \n",
    "- Creates an instance of the Logistic classifier class \n",
    "- The train function of the Logistic class is trained on the training data\n",
    "- We use the predict function to find the training accuracy as well as the testing accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Logistic Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2.5\n",
    "n_epochs = 1000\n",
    "threshold = 0.5\n",
    "\n",
    "lr = Logistic(learning_rate, n_epochs, threshold)\n",
    "\n",
    "# Refer to hints in the sigmoid and train methods of the Logistic class in \"models/logistic.py\"\n",
    "lr.train(X_train_RICE, y_train_RICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10911,)\n",
      "(10911,) (10911,)\n",
      "(10911,)\n",
      "8378\n",
      "8378 10911\n",
      "The training accuracy is given by: 76.784896\n"
     ]
    }
   ],
   "source": [
    "pred_lr = lr.predict(X_train_RICE)\n",
    "eq = y_train_RICE == pred_lr\n",
    "print(y_train_RICE.shape, pred_lr.shape)\n",
    "print(eq.shape)\n",
    "print(np.count_nonzero(eq))\n",
    "print('The training accuracy is given by: %f' % (get_acc(pred_lr, y_train_RICE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Logistic Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3637,)\n",
      "2832 3637\n",
      "The validation accuracy is given by: 77.866373\n"
     ]
    }
   ],
   "source": [
    "pred_lr = lr.predict(X_val_RICE)\n",
    "print('The validation accuracy is given by: %f' % (get_acc(pred_lr, y_val_RICE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Logistic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3637,)\n",
      "2790 3637\n",
      "The testing accuracy is given by: 76.711575\n"
     ]
    }
   ],
   "source": [
    "pred_lr = lr.predict(X_test_RICE)\n",
    "print('The testing accuracy is given by: %f' % (get_acc(pred_lr, y_test_RICE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
